#!/bin/bash

USER_ENV=`whoami`

echo "=== Arnold torch distributed launch script ==="
echo "=== contact: haibin.lin@bytedance.com ==="

set -x

# if [[ "$ARNOLD_ROLE" == "server" ]]; then
#   byteps_launch
#   exit
# fi

# if [[ "$USE_CRUISE_ELASTICRUN" == "1" ]]; then
#   master_addr=$MASTER_ADDR
#   master_port=$MASTER_PORT
#   nnodes=$NUM_NODES
#   node_rank=$NODE_RANK

#   echo "=== Cruise Elastic Training ==="
#   echo "=== contact: zhihaobai@bytedance.com ==="
#   echo "Elastic Training Info: Master address: ${master_addr}:${master_port}"
#   echo "Elastic Training Info: Rank ${node_rank} in ${nnodes} nodes"
# fi

# if [[ "$ARNOLD_TRIAL_ID" == "" ]]; then
#   nnodes=1
#   node_rank=0
#   trial_id=12345
#   NCCL_IB_DISABLE=1
#   nproc_per_node=${nproc_per_node:=$(nvidia-smi --list-gpus | wc -l)}
#   additional_args="--standalone"
# else
#   master_addr=${master_addr:=$ARNOLD_WORKER_0_HOST}
#   master_port=${master_port:=$(echo "$ARNOLD_WORKER_0_PORT" | cut -d "," -f 1)}
#   additional_args="--rdzv_endpoint=${master_addr}:${master_port}"
#   if [[ "$nnodes" == "1" ]]; then
#     additional_args="$additional_args --standalone"
#   fi
# fi

IP_ADDR=$(ip -4 addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')

# 根据 IP 地址设置 node_rank
case $IP_ADDR in
    "10.232.194.226")
        node_rank=0
        ;;
    "10.232.194.227")
        node_rank=1
        ;;
    "10.232.195.24")
        node_rank=2
        ;;
    "10.232.195.25")
        node_rank=3
        ;;
    *)
        echo "Unknown IP address: $IP_ADDR"
        exit 1
        ;;
esac
master_addr="10.232.194.226"
master_port="10086"
additional_args="--rdzv_endpoint=${master_addr}:${master_port}"
nproc_per_node="8"
nnodes="4"

# if [[ "$ARNOLD_DEVICE_TYPE" == *A100* ]]; then
#   IB_HCA=mlx5
# else
#   IB_HCA=$ARNOLD_RDMA_DEVICE:1
# fi

# if [ "$ARNOLD_RDMA_DEVICE" != "" ]; then
#    export NCCL_IB_DISABLE=${NCCL_IB_DISABLE:=0}
#    export NCCL_IB_HCA=${NCCL_IB_HCA:=$IB_HCA}
#    export NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX:=3}
#    export NCCL_SOCKET_IFNAME=${NCCL_SOCKET_IFNAME:=eth0}
# else
#    export NCCL_IB_DISABLE=${NCCL_IB_DISABLE:=1}
#    export NCCL_SOCKET_IFNAME=${NCCL_SOCKET_IFNAME:=eth0}
# fi

# # setup tensorboard server if applicable
# if [[ "$TENSORBOARD_LOGDIR" != "" ]]; then
#   tensorboard_default_port="${ARNOLD_TENSORBOARD_CURRENT_PORT:=6006}"
#   tensorboard_port="${TENSORBOARD_PORT:=$tensorboard_default_port}"
#   nohup tensorboard --logdir=${TENSORBOARD_LOGDIR} --port=$tensorboard_port --bind_all > tensorboard.log 2>&1 &
# fi


export CUDA_DEVICE_MAX_CONNECTIONS=1
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# export CUDA_VISIBLE_DEVICES=0,1,4,5,8,9,12,13,2,3,6,7,10,11,14,15
# export NCCL_MIN_NCHANNELS=4
# export NCCL_NET_PLUGIN=none
# export NCCL_SOCKET_IFNAME=eth0
# export NCCL_DEBUG_SUBSYS=""
# export NCCL_P2P_LEVEL=PHB
# export NCCL_DEBUG=WARN
# export NCCL_IB_GID_INDEX=3
# export NCCL_NCHANNELS_PER_NET_PEER=4
# export NCCL_SOCKET_FAMILY=AF_INET6
# export NCCL_IB_TIMEOUT=23
export NCCL_IB_DISABLE=0
# export NCCL_IB_RETRY_CNT=7
# export NCCL_TOPO_FILE=/var/run/nvidia-topologyd/virtualTopology_l20.xml
# export MSCCL_ALGO_DIR=/opt/tiger/xmls
# export LD_LIBRARY_PATH=/opt/tiger/msccl/build/lib:$LD_LIBRARY_PATH

torchrun \
  --node_rank=$node_rank \
  --nproc_per_node=$nproc_per_node \
  --nnodes=$nnodes \
  $additional_args $@
